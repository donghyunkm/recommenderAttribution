[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Recommender System Attribution with TracIn",
    "section": "",
    "text": "Introduction\nRecommender systems use machine learning models to find items (eg. movies, songs) that a user may like. Data available to recommender systems include past user-item interactions, similarity of user behavior, item features, user’s context, among other available information. We focus on sequential recommender models, which predict future items that a user may interact with based on that user’s interaction history. The performance of recommender systems has significant financial implications.\nThe goal of our study is to examine why certain recommendations are bad and find methods to improve them. Before this project was put on hold, I developed methods to identify “bad” training data. Once identified, a new model was trained with the “bad” data removed/corrected. Examples of “bad” data include users erroenously clicking on unrelated items and users missing relevant items. Because we assume that all user-item interactions reflect a user’s preference, “bad” data significantly affects the quality of recommendations.\n\n\nLong Short-Term Memory\nLong Short-Term Memory is a recurrent neural nework composed of a memory cell, input gate, output gate, and forget gate (Hochreiter and Schmidhuber 1997).\n\n\n\n\n\n\nFigure 1: LSTM overview (MathWorks, n.d.)\n\n\n\nThe memory cell remembers information over long sequences. The input gate decides how much of a new input should be remembered, the output gate decides how much of the information in the memory cell should influence the output, and the forget gate decides how much information to discard from the memory cell.\n\n\nSequential recommendation model\nWe use a fairly simple sequential recommender system based on a LSTM (Long Short-Term Memory) model.\n\n\n\nEmbedding Step\nLSTM step\n\n\n\n\n\n\n\n\n\n\n\nFirst, each item is mapped to a vector embedding. Next, fixed length sequences of vector embeddings are fed into a LSTM model. The last timestep index of the LSTM’s output is then fed into a fully connected layer to predict the next item a user may interact with.\n\n\nTracIn\nWe use TracIn (Pruthi et al. 2020) to identify “bad” training data. TracIn computes the influence of a training instance on a prediction made by the model. The method estimates the change in test loss when the training instance of interest is used to update model weights. The idealized notion of influence of a training instance on a prediction is defined as the total reduction in loss on test data \\(z'\\) whenever training data \\(z\\) is used. Training data that reduce loss are “proponents” and training data that increase loss are “opponents.” Using TracIn, “bad” data is equivalent to “opponents.”\n\\[\nTracInIdeal(z, z') = \\sum_{t: z_t = z} \\ell(w_t, z') - \\ell(w_{t+1}, z')\n\\]\nThe authors provide a practical implementation, using saved checkpoints.\n\\[\nTracInCP(z, z') = \\sum_{i = 1}^k \\eta \\nabla \\ell(w_{t_i}, z) \\cdot \\nabla \\ell(w_{t_i}, z')\n\\]\nThis formula is derived using a first order approximation \\[\n\\ell(w_{t+1}, z') = \\ell(w_t, z') + \\nabla \\ell(w_t, z') \\cdot (w_{t+1} - w_t) + O(||w_{t+1} - w_t||^2)\n\\]\nand change in parameter formula\n\\[\nw_{t+1} - w_t = -\\eta \\nabla \\ell(w_t, z_t)\n\\]\n\n\nData\nWe use the Movielens 1M Dataset for experiments (Maxwell 2015). The dataset contains 1 million ratings from 6000 users on 4000 movies with timestamps for each rating. For results shown below, we use a subset of the dataset: around 100000 training sequences.\n\n\nTracIn applied to Movielens\n\n\nCode for our implementation of TracIn and LSTM training can be found here.\nWe apply TracIn to Movielens and find proponents/opponents for several well-known movies.\n\n\n\n\n\n\n\n\nTest item\nTop 2 proponents\nTop 2 opponents\n\n\n\n\nReturn of the Jedi (1983)\nStar Wars (1977), Toy Story (1995)\nPretty Woman (1990), Mrs. Doubtfire (1993)\n\n\nStar Trek III: The Search for Spock (1984)\nStar Trek VI: The Undiscovered Country (1991), Speed (1994)\nSense and Sensibility (1995), Amadeus (1984)\n\n\nL.A. Confidential (1997)\nEnglish Patient, The (1996), Contact (1997)\nTwister (1996), Die Hard 2 (1990)\n\n\nCitizen Kane (1941)\nAmadeus (1984), Casablanca (1942)\nBatman Returns (1992), Batman Forever (1995)\n\n\nTop Gun (1986)\nJurassic Park (1993), Speed (1994)\nEnglish Patient, The (1996), Sense and Sensibility (1995)\n\n\nJaws (1975)\nAlien (1979), Schindler’s List (1993)\nLiar Liar (1997), Boogie Nights (1997)\n\n\nG.I. Jane (1997)\nAir Force One (1997), Contact (1997)\nFargo (1996), Return of the Jedi (1983)\n\n\n\n\n\nFrom a quick glimpse, these results seem to make sense: eg. “Star Wars” positively influencing predictions for “Return of the Jedi.”\n\n\nTraining new model with opponents removed\nWith opponents identified, we train a new model with 500 opponent training sequences removed and compare results to that of a model trained on the original data.\n\n\n\n\n\n\n\n\nTrain data\nTest MRR (mean reciprocal rank)\nTest Recall@10\n\n\n\n\nOriginal\n0.0238\n0.0452\n\n\n500 opponents removd\n0.0264\n0.0514\n\n\n\nWe see that test performance (test data is kept constant for both settings) improves once opponents are removed.\n\n\nFuture work\nI worked on this project during Fall 2022 - Spring 2023. This project was put on hold as my PhD mentor found another project more relevant to his research interests at the time: a project on safety guardrails for Vision-Language models.\nIf I were to revist this project, I am interested in applying TracIn (or other similar methods) to the medical domain. TracIn could be used to identify “opponent” medical images for computer vision tasks.\n\n\nCode repository: https://github.com/donghyunkm/recommenderAttribution\n\n\n\n\n\nReferences\n\nHochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long Short-Term Memory.” Neural Computation 9 (8): 1735–80.\n\n\nMathWorks. n.d. “Long Short-Term Memory Neural Networks.” https://www.mathworks.com/help/deeplearning/ug/long-short-term-memory-networks.html.\n\n\nMaxwell, H. 2015. “A, k.: The MovieLens Datasets.” ACM Transactions on Interactive Intelligent Systems (TiiS).\n\n\nPruthi, Garima, Frederick Liu, Satyen Kale, and Mukund Sundararajan. 2020. “Estimating Training Data Influence by Tracing Gradient Descent.” Advances in Neural Information Processing Systems 33: 19920–30."
  }
]